{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Julio García Salas - 22076**\n",
    "## **Sofía García - 22210**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hoja de trabajo #2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Preguntas Teóricas\n",
    "\n",
    "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
    "\n",
    "## 1. Defina el proceso de decisión de Markov (MDP) y explique sus componentes.\n",
    "\n",
    "Un **Proceso de Decisión de Markov** (MDP, por sus siglas en inglés) es un marco matemático utilizado para modelar la toma de decisiones en entornos estocásticos con recompensas. Un MDP se define mediante los siguientes componentes:\n",
    "\n",
    "- **Estados ($S$):** Conjunto finito o infinito de estados posibles en los que puede encontrarse el agente.\n",
    "- **Acciones ($A$):** Conjunto de acciones que el agente puede tomar en cada estado.\n",
    "- **Función de transición ($P(s' | s, a)$):** Probabilidad de que el sistema transicione del estado $s$ al estado $s'$ al tomar la acción $a$.\n",
    "- **Recompensa ($R(s, a, s')$):** Recompensa inmediata obtenida al realizar la acción $a$ en el estado $s$ y moverse al estado $s'$.\n",
    "- **Factor de descuento ($\\gamma \\in [0,1]$):** Parámetro que determina la importancia de las recompensas futuras.\n",
    "\n",
    "El objetivo en un MDP es encontrar una política óptima que maximice la recompensa acumulada esperada a lo largo del tiempo.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Diferencia entre política, evaluación de políticas, mejora de políticas e iteración de políticas en los MDP\n",
    "\n",
    "- **Política ($\\pi$):** Función que define la acción a tomar en cada estado. Puede ser determinística ($\\pi(s) = a$) o estocástica ($\\pi(a | s)$ da una distribución de probabilidad sobre acciones).\n",
    "- **Evaluación de políticas:** Proceso de calcular el valor de cada estado dado que se sigue una política específica, generalmente usando la ecuación de Bellman para la función de valor:\n",
    "  $$ V^{\\pi}(s) = \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\middle| s_0 = s, \\pi \\right] $$\n",
    "- **Mejora de políticas:** Proceso de actualizar la política para obtener una mejor política basada en la función de valor calculada.\n",
    "- **Iteración de políticas:** Algoritmo que alterna entre la evaluación de políticas y la mejora de políticas hasta converger a una política óptima.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Concepto de factor de descuento ($\\gamma$) en los MDP y su influencia en la toma de decisiones\n",
    "\n",
    "El **factor de descuento** $\\gamma$ es un valor en el rango $[0,1]$ que controla la importancia de las recompensas futuras en la toma de decisiones. Se usa en la función de valor esperada:\n",
    "\n",
    "$$ V^{\\pi}(s) = \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\right] $$\n",
    "\n",
    "### Influencia en la toma de decisiones:\n",
    "- **Si $\\gamma \\approx 0$:** Se priorizan recompensas inmediatas, ignorando recompensas futuras.\n",
    "- **Si $\\gamma \\approx 1$:** Se consideran las recompensas futuras con casi la misma importancia que las inmediatas.\n",
    "- Un valor muy alto de $\\gamma$ puede hacer que el aprendizaje sea lento y que las decisiones se basen en horizontes de tiempo muy largos.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Diferencia entre iteración de valores e iteración de políticas para resolver MDP\n",
    "\n",
    "### **Iteración de Valores**\n",
    "- Se actualiza la función de valor directamente usando la ecuación de Bellman:\n",
    "  $$ V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s, a) \\left[R(s, a, s') + \\gamma V_k(s')\\right] $$\n",
    "- Se repite hasta que $V(s)$ converge.\n",
    "- Luego, se extrae la política óptima como $\\pi^*(s) = \\arg\\max_a Q(s,a)$.\n",
    "\n",
    "### **Iteración de Políticas**\n",
    "- Se parte de una política inicial.\n",
    "- Se evalúa su función de valor.\n",
    "- Se mejora la política en base a la evaluación.\n",
    "- Se repite hasta converger.\n",
    "\n",
    "### **Diferencias clave**\n",
    "- **Iteración de valores** actualiza los valores sin definir una política hasta el final.\n",
    "- **Iteración de políticas** optimiza directamente la política en cada iteración.\n",
    "- Iteración de valores suele ser más estable, mientras que iteración de políticas converge más rápido en muchos casos.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Desafíos y limitaciones en la resolución de MDP a gran escala\n",
    "\n",
    "Algunos desafíos en MDP de gran escala incluyen:\n",
    "- **Dimensionalidad del estado y acción:** Si hay demasiados estados o acciones, almacenar y calcular las funciones de valor es computacionalmente costoso.\n",
    "- **Exploración vs. explotación:** Encontrar un equilibrio entre explorar nuevas estrategias y explotar las mejores estrategias conocidas.\n",
    "- **Tiempo de cómputo:** Algoritmos como la iteración de valores pueden ser ineficientes para problemas grandes.\n",
    "\n",
    "### **Enfoques para abordar estos desafíos:**\n",
    "- **Aproximación de funciones:** En lugar de almacenar valores exactos, se usan redes neuronales o modelos lineales.\n",
    "- **Métodos basados en muestreo:** Métodos como Monte Carlo y Q-learning evitan la necesidad de modelar toda la dinámica del MDP.\n",
    "- **Algoritmos jerárquicos:** Dividen el problema en subproblemas más manejables.\n",
    "- **Aprendizaje por refuerzo profundo:** Combinación de aprendizaje profundo con técnicas de RL para manejar espacios de estado masivos.\n",
    "\n",
    "Estos enfoques permiten aplicar MDPs en problemas complejos como juegos, robótica y optimización de recursos en tiempo real.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Preguntas Analíticas\n",
    "\n",
    "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
    "\n",
    "## 1. Análisis crítico de los supuestos subyacentes a la propiedad de Markov en los MDP\n",
    "\n",
    "El **supuesto de Markov** establece que el futuro depende únicamente del estado actual y no del historial de estados previos. Sin embargo, en escenarios reales, este supuesto puede no ser válido. Algunos ejemplos incluyen:\n",
    "\n",
    "- **Dependencia del historial:** En situaciones donde los efectos acumulativos del pasado afectan el futuro, como la fatiga en un atleta o el historial de crédito de un usuario.\n",
    "- **Estados parcialmente observables:** Cuando el agente no tiene acceso completo a la información relevante del entorno, lo que requiere enfoques como los POMDPs (Procesos de Decisión de Markov Parcialmente Observables).\n",
    "- **Variables ocultas:** Factores externos que influyen en la transición de estados pero que no están explícitamente representados en el modelo.\n",
    "\n",
    "Para abordar estas limitaciones, se pueden emplear enfoques como el uso de memoria a largo plazo, técnicas de inferencia y redes neuronales recurrentes (RNNs) para estimar estados ocultos.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Modelado de la incertidumbre en los MDP y estrategias de toma de decisiones sólida\n",
    "\n",
    "La incertidumbre en los MDPs puede surgir debido a la variabilidad en las transiciones de estado y recompensas. Algunas estrategias para manejar esta incertidumbre incluyen:\n",
    "\n",
    "- **Aprendizaje por refuerzo robusto:** Busca optimizar la política en el peor caso posible.\n",
    "- **Métodos de exploración y explotación:** Técnicas como Thompson Sampling o UCB (Upper Confidence Bound) ayudan a manejar la incertidumbre de manera efectiva.\n",
    "- **Modelos probabilísticos bayesianos:** Permiten actualizar las creencias sobre la dinámica del entorno a medida que se recopilan datos.\n",
    "\n",
    "Estas estrategias son fundamentales en aplicaciones como la robótica, el comercio financiero y la gestión de recursos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\garci\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\garci\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gym) (2.1.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\garci\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gym) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\garci\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gym) (0.0.8)\n",
      "Configuración del entorno:\n",
      "SFFF\n",
      "FFFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "Política Óptima:\n",
      "[['↓' '↓' '↓' '↓']\n",
      " ['↓' '↓' '↓' '↓']\n",
      " ['→' '↓' '↓' '↓']\n",
      " ['←' '→' '→' '←']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gym\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "if not hasattr(np, 'bool8'):\n",
    "    np.bool8 = np.bool_\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Función para crear un entorno personalizado de FrozenLake 4x4 con hoyos aleatorios\n",
    "def generate_frozen_lake():\n",
    "    size = 4\n",
    "    lake = np.full((size, size), 'F') \n",
    "    \n",
    "    # Definir posiciones de inicio y meta\n",
    "    start_positions = [(0, 0), (0, size-1), (size-1, 0), (size-1, size-1)]\n",
    "    start_pos = random.choice(start_positions)\n",
    "    lake[start_pos] = 'S'  \n",
    "\n",
    "    goal_pos = (size-1 - start_pos[0], size-1 - start_pos[1])\n",
    "    lake[goal_pos] = 'G'  \n",
    "\n",
    "    # Colocar entre 1 y 3 hoyos aleatoriamente\n",
    "    num_holes = random.randint(1, 3)\n",
    "    empty_spaces = [(i, j) for i in range(size) for j in range(size)\n",
    "                    if (i, j) != start_pos and (i, j) != goal_pos]\n",
    "    \n",
    "    holes = random.sample(empty_spaces, num_holes)\n",
    "    for hole in holes:\n",
    "        lake[hole] = 'H'  \n",
    "\n",
    "    desc = [\"\".join(row) for row in lake]\n",
    "    return desc, start_pos, goal_pos\n",
    "\n",
    "desc, start_pos, goal_pos = generate_frozen_lake()\n",
    "env = gym.make(\"FrozenLake-v1\", desc=desc, is_slippery=False)\n",
    "\n",
    "print(\"Configuración del entorno:\")\n",
    "for row in desc:\n",
    "    print(row)\n",
    "\n",
    "goal_state = goal_pos[0] * 4 + goal_pos[1]\n",
    "\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "gamma = 0.9 \n",
    "\n",
    "V = np.zeros(num_states)\n",
    "policy = np.zeros(num_states, dtype=int)\n",
    "\n",
    "threshold = 1e-4\n",
    "delta = float(\"inf\")\n",
    "while delta > threshold:\n",
    "    delta = 0\n",
    "    new_V = np.copy(V)\n",
    "    for s in range(num_states):\n",
    "        if s == goal_state:  \n",
    "            continue\n",
    "        q_values = []\n",
    "        for a in range(num_actions):\n",
    "            expected_value = sum(\n",
    "                prob * (reward + gamma * V[next_state])\n",
    "                for prob, next_state, reward, _ in env.P[s][a]\n",
    "            )\n",
    "            q_values.append(expected_value)\n",
    "        new_V[s] = max(q_values)\n",
    "        delta = max(delta, abs(V[s] - new_V[s]))\n",
    "    V = new_V\n",
    "\n",
    "for s in range(num_states):\n",
    "    q_values = []\n",
    "    for a in range(num_actions):\n",
    "        expected_value = sum(\n",
    "            prob * (reward + gamma * V[next_state])\n",
    "            for prob, next_state, reward, _ in env.P[s][a]\n",
    "        )\n",
    "        q_values.append(expected_value)\n",
    "    policy[s] = np.argmax(q_values)\n",
    "\n",
    "actions = [\"←\", \"↓\", \"→\", \"↑\"]\n",
    "policy_grid = np.array([actions[a] for a in policy]).reshape(4, 4)\n",
    "print(\"\\nPolítica Óptima:\")\n",
    "print(policy_grid)\n",
    "\n",
    "def run_agent(env, policy, max_steps=20):\n",
    "    state, _ = env.reset()\n",
    "    env.render()\n",
    "    for step in range(max_steps):\n",
    "        action = policy[state]\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "run_agent(env, policy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
